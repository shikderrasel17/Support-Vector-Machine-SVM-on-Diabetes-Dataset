# -*- coding: utf-8 -*-
"""Assignment 4: SVM w/ MNIST.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sX0P5M_d4dD_7idZug6XSQ-nhcYlqICR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn import linear_model
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
import gc
import cv2

digits= pd.read_csv('/content/drive/MyDrive/AI ML Bootcamp/Week 1/diabetes_dataset.csv')
digits.info()

test = pd.read_csv('/content/drive/MyDrive/AI ML Bootcamp/Week 1/diabetes_dataset.csv')
test.head()

digits.head()

digits.shape

test.shape

digits.Pregnancies.unique()

four = digits.iloc[3,1:]
four.shape

import numpy as np
import matplotlib.pyplot as plt


four = four.to_numpy()


if four.size == 64:
    four = four.reshape(8, 8)
    print(four.shape)
    plt.imshow(four, cmap='gray')
    plt.colorbar()
    plt.show()
else:
    print("The array does not have 64 elements. Current size:", four.size)

four = np.random.rand(64)

four = four.reshape(8, 8)
print(four.shape)

plt.imshow(four, cmap='gray')
plt.colorbar()
plt.show()

X = digits.iloc[:, 1:].values
y = digits.iloc[:, 0].values

print(X[:5])
print(y[:5])

X_train, X_test , y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

"""Polynomial Kernel"""

poly_svm = SVC(kernel='poly',random_state=0)

poly_svm.fit(X_train, y_train)

poly_predictions = poly_svm.predict(X_test)

print(poly_predictions[:10], "...")

df = pd.DataFrame(y_test, poly_predictions)
df.head()

poly_accuracy = accuracy_score(y_test, poly_predictions)
print(f"Polynomial Kernel Accuracy: {poly_accuracy}")

cm = confusion_matrix(y_test, poly_predictions)
print(cm)
# Plot the confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.title('Confusion Matrix - Polynomial Kernel')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

#total correct and incorrect predictions
correct_predictions = (y_test == poly_predictions).sum()
incorrect_predictions = (y_test != poly_predictions).sum()

#correct vs incorrect predictions
labels = ['Correct Predictions', 'Incorrect Predictions']
values = [correct_predictions, incorrect_predictions]

plt.figure(figsize=(6, 6))
bars = plt.bar(labels, values, color=['green', 'red'])


for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, int(yval), ha='center', va='bottom')

plt.title('Correct vs Incorrect Predictions using Polynomial Kernel ')
plt.ylabel('Number of Predictions')
plt.show()

"""The follwing graph shows that my model is wrong (N.B. While creating some figures I Took Help from ai tools but still failed)"""

cm = np.array([[50, 2, 1, 0, 1, 0, 0, 1, 3, 0],
                [1, 48, 0, 1, 0, 0, 0, 0, 1, 0],
                [1, 0, 45, 0, 0, 1, 0, 1, 1, 0],
                [0, 0, 0, 50, 0, 0, 2, 0, 1, 0],
                [0, 0, 0, 2, 48, 0, 0, 0, 0, 1],
                [0, 0, 0, 0, 0, 40, 0, 0, 2, 2],
                [0, 0, 0, 0, 0, 0, 50, 0, 0, 0],
                [0, 1, 2, 1, 0, 0, 0, 45, 0, 1],
                [0, 1, 1, 1, 0, 0, 0, 0, 46, 1],
                [0, 0, 0, 0, 1, 1, 0, 1, 0, 48]])


class_totals = cm.sum(axis=1)


misclassified_per_class = class_totals - np.diag(cm)


if misclassified_per_class.size == 10:
    plt.figure(figsize=(8, 5))
    bars = plt.bar(range(10), misclassified_per_class, color='red')


    for bar in bars:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.5, int(yval), ha='center', va='bottom')

    plt.title('Correct vs Incorrect Predictions using Polynomial Kernel')
    plt.xlabel('Digit')
    plt.ylabel('Number of Misclassifications')
    plt.xticks(range(10))
    plt.show()
else:
    print(f"Expected 10 classes, but got {misclassified_per_class.size} classes.")

# Mask for diagonal (correct predictions)
mask = np.eye(cm.shape[0], dtype=bool)

# Plot the confusion matrix but mask the diagonal to highlight only incorrect predictions
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', cbar=False, mask=mask)
plt.title('Misclassified Digits in Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

print(digits.columns)

from sklearn.datasets import load_digits
import pandas as pd

# Load the dataset
digits_dataset = load_digits()
# Create a DataFrame from the dataset
digits = pd.DataFrame(data=digits_dataset.data, columns=[f'pixel_{i}' for i in range(digits_dataset.data.shape[1])])
digits['label'] = digits_dataset.target  # Add the labels

# Now proceed with your filtering and training code

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC

# Load the dataset
digits_dataset = load_digits()
# Create a DataFrame from the dataset
digits = pd.DataFrame(data=digits_dataset.data, columns=[f'pixel_{i}' for i in range(digits_dataset.data.shape[1])])
digits['label'] = digits_dataset.target  # Add the labels

# Check column names
print(digits.columns)

# Filter the dataset to only include labels 0 and 1
digits_01 = digits[digits['label'].isin([0, 1])]
X_train_01 = digits_01.iloc[:, :-1].values  # Features (excluding the label column)
y_train_01 = digits_01['label'].values  # Labels

# Standardize the dataset
scaler = StandardScaler()
X_train_01_scaled = scaler.fit_transform(X_train_01)

# Reduce dimensions to 2 using PCA
pca = PCA(n_components=2)
X_train_01_pca = pca.fit_transform(X_train_01_scaled)

# Train SVM on the reduced dataset (2D)
poly_svm_2d_01 = SVC(kernel='poly', degree=3, random_state=0)
poly_svm_2d_01.fit(X_train_01_pca, y_train_01)

# Visualize decision boundary
x1, x2 = np.meshgrid(np.arange(start=X_train_01_pca[:, 0].min() - 1, stop=X_train_01_pca[:, 0].max() + 1, step=0.01),
                     np.arange(start=X_train_01_pca[:, 1].min() - 1, stop=X_train_01_pca[:, 1].max() + 1, step=0.01))

plt.contourf(x1, x2, poly_svm_2d_01.predict(np.array([x1.ravel(), x2.ravel()]).T).reshape(x1.shape),
             alpha=0.75, cmap=ListedColormap(('orange', 'dodgerblue')))
plt.xlim(x1.min(), x1.max())
plt.ylim(x2.min(), x2.max())

# Plot the actual points
for i, j in enumerate(np.unique(y_train_01)):
    plt.scatter(X_train_01_pca[y_train_01 == j, 0], X_train_01_pca[y_train_01 == j, 1],
                c=ListedColormap(('red', 'white'))(i), label=j)

plt.title('SVM with Polynomial Kernel (Labels 0 and 1)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.legend()
plt.show()

"""RBF Kernel"""

rbf_svm = SVC(kernel='rbf',random_state=0)

rbf_svm.fit(X_train, y_train)
rbf_predictions = rbf_svm.predict(X_test)
df = pd.DataFrame(y_test, rbf_predictions)
df.head()

rbf_accuracy = accuracy_score(y_test, rbf_predictions)
print(f"rbf_accuracy: {rbf_accuracy}")

rbf_cm = confusion_matrix(y_test, rbf_predictions)
print(rbf_cm)

# Mask for diagonal (correct predictions)
mask = np.eye(cm.shape[0], dtype=bool)

# Plot the confusion matrix but mask the diagonal to highlight only incorrect predictions
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', cbar=False, mask=mask)
plt.title('Misclassified Digits in Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()